{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3378be43",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from frame_semantic_transformer import FrameSemanticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_transformer_base = FrameSemanticTransformer('base', batch_size=50)\n",
    "frame_transformer_base.model.to('cuda:0')\n",
    "frame_transformer_small = FrameSemanticTransformer('small', batch_size=16)\n",
    "frame_transformer_small.model.to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a71e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_transformer_base = FrameSemanticTransformer(batch_size=16)\n",
    "\n",
    "result = frame_transformer_base.detect_frames_bulk([\n",
    "            'I saw a movie yesterday',\n",
    "            'We own a house',\n",
    "            'We borrow money',\n",
    "            'Mom sends a letter',\n",
    "            'The child drinks milk',\n",
    "            'She tells the truth',\n",
    "            'I know the answer',\n",
    "            'The car stops here',\n",
    "            'He opens the window',\n",
    "            'We go to the park',\n",
    "            'The train arrives now',\n",
    "            'The book is on the shelf',\n",
    "            'I think of you',\n",
    "            'He loves his dog',\n",
    "            'She understands the question',\n",
    "            'We forget the name',\n",
    "            'They talk about the weather',\n",
    "            'The teacher answers the student',\n",
    "            'Dad promises a present',\n",
    "            'She calls her friend',\n",
    "            'Ice melts quickly',\n",
    "            'The sun shines today',\n",
    "            'He slams the door',\n",
    "            'He writes an email',\n",
    "            'The meeting starts at nine oclock',\n",
    "            'I turn on the TV',\n",
    "            'It is raining outside',\n",
    "            'The child sleeps deeply',\n",
    "            'She comes up with the answer',\n",
    "            'She gives up',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d708380",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_lemmas = ['attack_nn', 'bag_nn', 'ball_nn', 'bit_nn', 'chairman_nn', 'circle_vb', 'contemplation_nn', 'donkey_nn', 'edge_nn', 'face_nn', 'fiction_nn', 'gas_nn', 'graft_nn', 'head_nn', 'land_nn', 'lane_nn', 'lass_nn', 'multitude_nn', 'ounce_nn', 'part_nn', 'pin_vb', 'plane_nn', 'player_nn', 'prop_nn', 'quilt_nn', 'rag_nn', 'record_nn', 'relationship_nn', 'risk_nn', 'savage_nn', 'stab_nn', 'stroke_vb', 'thump_nn', 'tip_vb', 'tree_nn', 'twist_nn', 'word_nn']\n",
    "\n",
    "mode = 'token' # 'token' or 'lemma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ff5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_lemma in selected_lemmas:\n",
    "    print(f\"Processing lemma: {selected_lemma}\")\n",
    "\n",
    "    # Load corpus data\n",
    "    corpus_1_path = f'... /SemEval_en_split/corpus1/{mode}/ccoha1_{selected_lemma}.csv' # <-- Replace with actual path\n",
    "    df = pd.read_csv(corpus_1_path, dtype=str, keep_default_na=False)\n",
    "    corpus_1 = df[\"sent\"].astype(str).str.strip()\n",
    "    corpus_1 = corpus_1[corpus_1 != \"\"].tolist()\n",
    "\n",
    "    corpus_2_path = f'... /SemEval_en_split/corpus2/{mode}/ccoha2_{selected_lemma}.csv' # <-- Replace with actual path\n",
    "    df = pd.read_csv(corpus_2_path, dtype=str, keep_default_na=False)\n",
    "    corpus_2 = df[\"sent\"].astype(str).str.strip()\n",
    "    corpus_2 = corpus_2[corpus_2 != \"\"].tolist()\n",
    "\n",
    "    # Parse the data\n",
    "    corpora = {\n",
    "    \"corpus_1\": corpus_1,\n",
    "    \"corpus_2\": corpus_2,\n",
    "    }\n",
    "\n",
    "    total_results = {name: [] for name in corpora}\n",
    "\n",
    "    for name, sentences in corpora.items():\n",
    "        base_results = frame_transformer_base.detect_frames_bulk(sentences)  # sentences are list[str]\n",
    "        for base_result in base_results:\n",
    "            org_sent = base_result.sentence\n",
    "\n",
    "            if not base_result.frames:\n",
    "                print(f\"No frames detected for {org_sent}.\\nParse again with small model.\")\n",
    "                small_result = frame_transformer_small.detect_frames(org_sent)\n",
    "                if not small_result.frames:\n",
    "                    print(f\"Still no frames detected for {org_sent}.\\n\")\n",
    "                    continue\n",
    "                else: # If small model detects frames, use its result\n",
    "                    total_results[name].append(small_result)\n",
    "            else:\n",
    "                total_results[name].append(base_result)\n",
    "\n",
    "    # Save results to txt\n",
    "    output_path = f'... /{selected_lemma}_{mode}_FrameNet_parsed.txt' # <-- Replace with actual path\n",
    "    with open(output_path, 'w') as f:\n",
    "        for name, results in total_results.items():\n",
    "            f.write(f\"=== Results for {name} ===\\n\\n\")\n",
    "            for result in results:\n",
    "                f.write(str(result))\n",
    "                f.write(\"\\n\\n\")\n",
    "    print(f\"Finished processing lemma: {selected_lemma}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing code\n",
    "import pandas as pd\n",
    "corpus_1_path = '... corpus1/token/ccoha1_multitude_nn.csv'\n",
    "df = pd.read_csv(corpus_1_path, dtype=str, keep_default_na=False)\n",
    "corpus_1 = df[\"sent\"].astype(str).str.strip()\n",
    "corpus_1 = corpus_1[corpus_1 != \"\"].tolist()\n",
    "\n",
    "corpus_2_path = '... /corpus2/token/ccoha2_multitude_nn.csv'\n",
    "df = pd.read_csv(corpus_2_path, dtype=str, keep_default_na=False)\n",
    "corpus_2 = df[\"sent\"].astype(str).str.strip()\n",
    "corpus_2 = corpus_2[corpus_2 != \"\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240be7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\n",
    "    \"corpus_1\": corpus_1,\n",
    "    \"corpus_2\": corpus_2,\n",
    "}\n",
    "\n",
    "total_results = {name: [] for name in corpora}\n",
    "\n",
    "for name, sentences in corpora.items():\n",
    "    base_results = frame_transformer_base.detect_frames_bulk(sentences)  # sentences are list[str]\n",
    "    for base_result in base_results:\n",
    "        org_sent = base_result.sentence\n",
    "\n",
    "        if not base_result.frames:\n",
    "            print(f\"No frames detected for {org_sent}.\\nParse again with small model.\")\n",
    "            small_result = frame_transformer_small.detect_frames(org_sent)\n",
    "            if not small_result.frames:\n",
    "                print(f\"Still no frames detected for {org_sent}.\\n\")\n",
    "                continue\n",
    "            else: # If small model detects frames, use its result\n",
    "                total_results[name].append(small_result)\n",
    "        else:\n",
    "            total_results[name].append(base_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14251e8a",
   "metadata": {},
   "source": [
    "## Compute skipped sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8652b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def count_skipped_in_jsonl(\n",
    "    jsonl_path: Path,\n",
    "    corpus_field: str = \"corpus\",\n",
    "    trigger_field: str = \"trigger_locations\",\n",
    ") -> dict[str, dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Returns per-corpus totals and skipped counts:\n",
    "      {\n",
    "        \"corpus_1\": {\"total\": N, \"skipped\": K},\n",
    "        \"corpus_2\": {\"total\": N, \"skipped\": K},\n",
    "      }\n",
    "\n",
    "    skipped := trigger_locations == []\n",
    "    \"\"\"\n",
    "    out: dict[str, dict[str, int]] = {}\n",
    "\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON on line {line_no} in {jsonl_path}: {e}\") from e\n",
    "\n",
    "            corpus = obj.get(corpus_field) or \"MISSING_CORPUS_FIELD\"\n",
    "            out.setdefault(corpus, {\"total\": 0, \"skipped\": 0})\n",
    "\n",
    "            out[corpus][\"total\"] += 1\n",
    "            if obj.get(trigger_field, None) == []:\n",
    "                out[corpus][\"skipped\"] += 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def lemma_from_filename(p: Path) -> str:\n",
    "    \"\"\"\n",
    "    Expect: <lemma>_<mode>_FrameNet_parsed.jsonl\n",
    "    Example: attack_nn_lemma_FrameNet_parsed.jsonl -> attack_nn\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+?)_(lemma|token)_FrameNet_parsed\\.jsonl$\", p.name)\n",
    "    return m.group(1) if m else p.stem\n",
    "\n",
    "\n",
    "def build_skipped_table(\n",
    "    base_dir: str | Path,\n",
    "    modes: list[str] = [\"lemma\", \"token\"],\n",
    "    corpus_names: list[str] = [\"corpus_1\", \"corpus_2\"],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scans:\n",
    "      {base_dir}/{mode}/*_FrameNet_parsed.jsonl\n",
    "\n",
    "    Row per (mode, lemma, corpus).\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    rows = []\n",
    "\n",
    "    for mode in modes:\n",
    "        d = base_dir / mode\n",
    "        if not d.exists():\n",
    "            continue\n",
    "\n",
    "        for jsonl_path in sorted(d.glob(\"*_FrameNet_parsed.jsonl\")):\n",
    "            lemma = lemma_from_filename(jsonl_path)\n",
    "            stats = count_skipped_in_jsonl(jsonl_path)\n",
    "\n",
    "            for corpus in corpus_names:\n",
    "                total = stats.get(corpus, {}).get(\"total\", 0)\n",
    "                skipped = stats.get(corpus, {}).get(\"skipped\", 0)\n",
    "                rate = (skipped / total) if total else 0.0\n",
    "\n",
    "                rows.append({\n",
    "                    \"mode\": mode,\n",
    "                    \"lemma\": lemma,\n",
    "                    \"corpus\": corpus,\n",
    "                    \"total\": total,\n",
    "                    \"skipped\": skipped,\n",
    "                    \"skipped_rate\": rate,\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def summary_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only counts (no averages):\n",
    "    - per (mode, corpus): n_lemmas, missed_total, total_sentences\n",
    "    - overall per mode (corpus=ALL): sums across corpora, n_lemmas (unique lemmas in that mode)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"mode\", \"corpus\", \"n_lemmas\", \"missed_total\", \"total_sentences\"])\n",
    "\n",
    "    per_corpus = (\n",
    "        df.groupby([\"mode\", \"corpus\"], as_index=False)\n",
    "          .agg(\n",
    "              n_lemmas=(\"lemma\", \"nunique\"),\n",
    "              missed_total=(\"skipped\", \"sum\"),\n",
    "              total_sentences=(\"total\", \"sum\"),\n",
    "          )\n",
    "    )\n",
    "\n",
    "    overall = (\n",
    "        df.groupby([\"mode\"], as_index=False)\n",
    "          .agg(\n",
    "              n_lemmas=(\"lemma\", \"nunique\"),\n",
    "              missed_total=(\"skipped\", \"sum\"),\n",
    "              total_sentences=(\"total\", \"sum\"),\n",
    "          )\n",
    "    )\n",
    "    overall[\"corpus\"] = \"ALL\"\n",
    "    overall = overall[[\"mode\", \"corpus\", \"n_lemmas\", \"missed_total\", \"total_sentences\"]]\n",
    "\n",
    "    return pd.concat([per_corpus, overall], ignore_index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE = \"...\" # <-- Path to the parsed data\n",
    "\n",
    "    df = build_skipped_table(BASE, modes=[\"lemma\", \"token\"], corpus_names=[\"corpus_1\", \"corpus_2\"])\n",
    "    df = df.sort_values([\"mode\", \"lemma\", \"corpus\"], kind=\"stable\")\n",
    "\n",
    "    summary = summary_counts(df).sort_values([\"mode\", \"corpus\"], kind=\"stable\")\n",
    "\n",
    "    # In hết rows (không head 20)\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 200)\n",
    "\n",
    "    print(\"=== Per-lemma skipped table (ALL rows) ===\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== Total miss (skipped_rate) ===\")\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    # Save nếu cần\n",
    "    out_dir = Path(BASE) / \"_reports\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_dir / \"skipped_table.csv\", index=False)\n",
    "    summary.to_csv(out_dir / \"skipped_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff2cc4",
   "metadata": {},
   "source": [
    "# Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21462407",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lucyYB/SweFN-SRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from frame_semantic_transformer import FrameSemanticTransformer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom Traning and Inference loaders for the Swedish Framenet data\n",
    "# This is the core step necessary to get FrameSemanticTransformer to work with different languages/framenets\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import re\n",
    "from typing import List\n",
    "from frame_semantic_transformer.data.loaders.loader import TrainingLoader, InferenceLoader\n",
    "from frame_semantic_transformer.data.frame_types import Frame, FrameAnnotatedSentence, FrameAnnotation, FrameElementAnnotation\n",
    "from frame_semantic_transformer.data.augmentations import (\n",
    "    DataAugmentation,\n",
    "    LowercaseAugmentation,\n",
    "    RemoveEndPunctuationAugmentation,\n",
    ")\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "swedish_stemmer = SnowballStemmer(\"swedish\")\n",
    "\n",
    "\n",
    "def extract_frame(xml_frame) -> Frame:\n",
    "    \"\"\"\n",
    "    Extract a Frame element from the Swedish XML for a frame\n",
    "    \"\"\"\n",
    "    name = xml_frame.attrib[\"id\"].replace('swefn--', '')\n",
    "    core_elms = [\n",
    "        ft.attrib[\"val\"] for ft in xml_frame.findall(\".//feat[@att='coreElement']\")\n",
    "    ]\n",
    "    non_core_elms = [\n",
    "        ft.attrib[\"val\"]\n",
    "        for ft in xml_frame.findall(\".//feat[@att='peripheralElement']\")\n",
    "    ]\n",
    "    lus = [ft.attrib[\"val\"] for ft in xml_frame.findall(\".//feat[@att='LU']\")]\n",
    "    # some examples have triggers outside the listed 'LU', but they are usually registerred as 'suggestionForLU'\n",
    "    lus += [ft.attrib[\"val\"] for ft in xml_frame.findall(\".//feat[@att='suggestionForLU']\")]\n",
    "\n",
    "    return Frame(\n",
    "        name=name,\n",
    "        core_elements=core_elms,\n",
    "        non_core_elements=non_core_elms,\n",
    "        lexical_units=lus,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_example(example_xml, frame_name) -> FrameAnnotatedSentence:\n",
    "    \"\"\"\n",
    "    Extract an annotated training sentence from a Swedish FrameNet Example XML\n",
    "    NOTE: This isn't ideal since only 1 frame is tagged in each example. This may\n",
    "    cause the Swedish FrameSemanticTransformer to only ever tag 1 frame per sentence.\n",
    "    \"\"\"\n",
    "    nodes_to_extract = [n for n in example_xml]\n",
    "    text = \"\"\n",
    "    trigger_locs = []\n",
    "    frame_elements = []\n",
    "    while len(nodes_to_extract) > 0:\n",
    "        node = nodes_to_extract.pop(0)\n",
    "        # sometimes there's nodes in nodes, compound annotation in SweFN\n",
    "        # in this case, push the children of this node to the front of the queue and keep going\n",
    "        if len(node) > 0:\n",
    "            nodes_to_extract = [n for n in node] + nodes_to_extract\n",
    "        else:\n",
    "            cur_index = len(text)\n",
    "            if not node.text:\n",
    "                continue\n",
    "            node_text = re.sub(r\"\\s+\", ' ', node.text)\n",
    "            if node.attrib.get(\"name\") == \"LU\":\n",
    "                trigger_locs.append(cur_index)\n",
    "            elif \"name\" in node.attrib:\n",
    "                frame_elements.append(\n",
    "                    FrameElementAnnotation(\n",
    "                        name=node.attrib[\"name\"],\n",
    "                        start_loc=cur_index,\n",
    "                        end_loc=cur_index + len(node_text),\n",
    "                    )\n",
    "                )\n",
    "            text += node_text + \" \"\n",
    "    text = text.strip()\n",
    "    return FrameAnnotatedSentence(\n",
    "        text=text,\n",
    "        annotations=[\n",
    "            FrameAnnotation(\n",
    "                frame=frame_name,\n",
    "                trigger_locs=trigger_locs,\n",
    "                frame_elements=frame_elements,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "class SwedishTrainingLoader(TrainingLoader):\n",
    "    \"\"\"\n",
    "    Training Loader for Swedish\n",
    "    This class tells FrameSemanticTransformer how to load the Swedish FrameNet training data\n",
    "    \"\"\"\n",
    "    train_sentences: List[FrameAnnotatedSentence]\n",
    "    test_sentences: List[FrameAnnotatedSentence]\n",
    "    val_sentences: List[FrameAnnotatedSentence]\n",
    "\n",
    "\n",
    "    def __init__(self, swedish_framenet_xml_file, test_portion=0.1, val_portion=0.1, seed=42):\n",
    "        # parse annotated sentences from XML\n",
    "        annotated_sentences = []\n",
    "        tree = ET.parse(swedish_framenet_xml_file)\n",
    "        root = tree.getroot()\n",
    "        for xml_frame in root.findall(\".//Sense\"):\n",
    "            frame = extract_frame(xml_frame)\n",
    "            for child in xml_frame:\n",
    "                if 'example' in child.tag:\n",
    "                    annotated_sentences.append(extract_example(child, frame.name))\n",
    "        # split into train/test/val \n",
    "        random.Random(seed).shuffle(annotated_sentences)\n",
    "        num_test = int(test_portion * len(annotated_sentences))\n",
    "        num_val = int(val_portion * len(annotated_sentences))\n",
    "\n",
    "        self.test_sentences = annotated_sentences[0:num_test]\n",
    "        self.val_sentences = annotated_sentences[num_test:num_test + num_val]\n",
    "        self.train_sentences = annotated_sentences[num_test + num_val:]\n",
    "    \n",
    "    def load_training_data(self):\n",
    "        return self.train_sentences\n",
    "    \n",
    "    def load_validation_data(self):\n",
    "        return self.val_sentences\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        return self.test_sentences\n",
    "    \n",
    "    def get_augmentations(self) -> List[DataAugmentation]:\n",
    "        \"\"\"\n",
    "        These augmentations try to increase the training data by making safe tweaks to the text\n",
    "        For instance, removing the punctuation at the end, or lowercasing the whole sentence\n",
    "        \"\"\"\n",
    "        return [\n",
    "            RemoveEndPunctuationAugmentation(0.3),\n",
    "            LowercaseAugmentation(0.2),\n",
    "        ]\n",
    "\n",
    "\n",
    "class SwedishInferenceLoader(InferenceLoader):\n",
    "    \"\"\"\n",
    "    Inference loader for Swedish\n",
    "    This class tells FrameSemanticTransformer which frames and LUs are available during inference\n",
    "    \"\"\"\n",
    "\n",
    "    frames: List[Frame]\n",
    "\n",
    "    def __init__(self, swedish_framenet_xml_file, test_portion=0.1, val_portion=0.1, seed=42):\n",
    "        # parse annotated sentences from XML\n",
    "        self.frames = []\n",
    "        tree = ET.parse(swedish_framenet_xml_file)\n",
    "        root = tree.getroot()\n",
    "        for xml_frame in root.findall(\".//Sense\"):\n",
    "            frame = extract_frame(xml_frame)\n",
    "            self.frames.append(frame)\n",
    "\n",
    "    def load_frames(self):\n",
    "        return self.frames\n",
    "    \n",
    "    def normalize_lexical_unit_text(self, lu):\n",
    "        \"\"\"\n",
    "        This method normalizes lexical unit text for Swedish during inference\n",
    "        Lexical Units help give hints to the model about what frames are likely \n",
    "        \"\"\"\n",
    "        normalized_lu = lu.lower()\n",
    "        normalized_lu = re.sub(r\"\\..+$\", \"\", normalized_lu)\n",
    "        normalized_lu = re.sub(r\"[^a-ö ]\", \" \", normalized_lu)\n",
    "        \n",
    "        ##### try 2\n",
    "        return \"_\".join([swedish_stemmer.stem(word) for word in normalized_lu.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_semantic_transformer import FrameSemanticTransformer\n",
    "\n",
    "# Path to tới XML SweFN (bắt buộc)\n",
    "SWEFN_XML = (\"... /SweFN-SRL/Model1/frame-semantic-transformer/swefn.xml\")   # <-- Replace with actual path\n",
    "\n",
    "loader = SwedishInferenceLoader(SWEFN_XML)\n",
    "\n",
    "# Load checkpoint + inference_loader\n",
    "custom_transformer = FrameSemanticTransformer(\n",
    "    \"... /models/frame_swe_m1_base\", # <-- Replace with actual path\n",
    "    inference_loader=loader,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "custom_transformer.model.to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "swe_test_sentences = ['Jag såg en film igår', # I watched a movie yesterday\n",
    "                      'Vi äger ett hus', # We own a house\n",
    "                      'Vi lånar pengar', # We borrow money\n",
    "                      'Mamma skickar ett brev', # Mom sends a letter\n",
    "                      'Barnet dricker mjölk', # The child drinks milk\n",
    "                      'Hon säger sanningen', # She tells the truth  \n",
    "                      'Jag vet svaret', # I know the answer\n",
    "                      'Bilen stannar här', # The car stops here\n",
    "                      'Han öppnar fönstret', # He opens the window\n",
    "                      'Vi går till parken', # We go to the park\n",
    "                      'Tåget anländer nu', # The train arrives now\n",
    "                      'Boken ligger på hyllan', # The book is on the shelf\n",
    "                      'Jag tänker på dig ', # I think of you\n",
    "                      'Han älskar sin hund', # He loves his dog\n",
    "                      'Hon förstår frågan', # She understands the question\n",
    "                      'Vi glömmer namnet', # We forget the name\n",
    "                      'De pratar om vädret', # They talk about the weather\n",
    "                      'Läraren svarar eleven', # The teacher answers the student\n",
    "                      'Pappa lovar en present', # Dad promises a present\n",
    "                      'Hon ringer sin vän', # She calls her friend\n",
    "                      'Isen smälter snabbt', # The ice melts quickly\n",
    "                      'Solen skiner idag', # The sun is shining today\n",
    "                      'Han slår igen dörren', # He slams the door shut\n",
    "                      'Han skriver ett mejl', # He writes an email\n",
    "                      'Mötet börjar klockan nio', # The meeting starts at nine o'clock\n",
    "                      'Jag slår på TV:n', # I turn on the TV\n",
    "                      'Det regnar ute', # It's raining outside\n",
    "                      'Barnet sover djupt', # The child is sleeping deeply\n",
    "                      'Hon kommer på svaret', # She comes up with the answer\n",
    "                      'Hon ger upp' # She gives up\n",
    "]\n",
    "\n",
    "res = custom_transformer.detect_frames_bulk(swe_test_sentences)\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
