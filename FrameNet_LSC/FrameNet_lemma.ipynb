{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3378be43",
   "metadata": {},
   "source": [
    "# Import and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distribution of frames for each period\n",
    "\n",
    "def get_frame_distribution(df, target_lemma):\n",
    "\n",
    "    span = len(target_lemma)\n",
    "    corpora = sorted(df[\"corpus\"].dropna().unique())\n",
    "    frames_with_lemma = {}\n",
    "\n",
    "    re_pattern = re.compile(rf\"(?<!\\w){re.escape(target_lemma.lower())}(?!\\w)\")\n",
    "\n",
    "    for corpus in corpora:\n",
    "        corpus_df = df[df['corpus'] == corpus]\n",
    "        frames_with_lemma[corpus] = []\n",
    "        \n",
    "        for _, row in corpus_df.iterrows():\n",
    "            org_sent = (row.get(\"sentence\") or \"\")\n",
    "            frames = row.get(\"frames\") or []\n",
    "            \n",
    "            for frame in frames:\n",
    "                # ATTENTION: To obtain trigger only or frame element only, simply comment out one of the two sections below.\n",
    "                \n",
    "                # Add frame if the target word is the trigger of the frame\n",
    "                trigger_start = frame.get(\"trigger_location\", None)\n",
    "                if isinstance(trigger_start, int):\n",
    "                    trigger_end = trigger_start + span\n",
    "                    if org_sent[trigger_start:trigger_end].lower() == target_lemma.lower():\n",
    "                        frames_with_lemma[corpus].append(frame.get(\"name\"))\n",
    "                        continue\n",
    "                \n",
    "                # Add frame if the target word appears in any of the frame elements\n",
    "                for element in (frame.get(\"frame_elements\") or []):\n",
    "                    txt = (element.get(\"text\") or \"\").lower()\n",
    "                    if re_pattern.search(txt):\n",
    "                        frames_with_lemma[corpus].append(frame.get(\"name\"))\n",
    "                        break\n",
    "\n",
    "    return frames_with_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a237501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_frames_target(df, target_lemma, output_csv=None):\n",
    "    if output_csv is None:\n",
    "        raise ValueError(\"output_csv must be provided\")\n",
    "\n",
    "    target = (target_lemma or \"\").lower()\n",
    "    span = len(target)\n",
    "\n",
    "    # word-boundary an toàn hơn \\b trong một số trường hợp\n",
    "    re_pattern = re.compile(rf\"(?<!\\w){re.escape(target)}(?!\\w)\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        org_sent = (row.get(\"sentence\") or \"\")\n",
    "        org_sent_l = org_sent.lower()\n",
    "\n",
    "        frames = row.get(\"frames\") or []\n",
    "        target_lemma_frames = []\n",
    "\n",
    "        for frame in frames:\n",
    "            # ATTENTION: To obtain trigger only or frame element only, simply comment out one of the two sections below.\n",
    "\n",
    "            # target is trigger\n",
    "            trigger_start = frame.get(\"trigger_location\", None)\n",
    "            if isinstance(trigger_start, int) and 0 <= trigger_start <= len(org_sent) - span:\n",
    "                trigger_end = trigger_start + span\n",
    "                if org_sent_l[trigger_start:trigger_end] == target:\n",
    "                    target_lemma_frames.append(frame.get(\"name\"))\n",
    "                    continue\n",
    "\n",
    "            # target is amongst the frame elements\n",
    "            for element in (frame.get(\"frame_elements\") or []):\n",
    "                txt = (element.get(\"text\") or \"\").lower()\n",
    "                if re_pattern.search(txt):\n",
    "                    target_lemma_frames.append(frame.get(\"name\"))\n",
    "                    break\n",
    "\n",
    "        records.append({\n",
    "            \"subfolder\": row.get(\"corpus\"),\n",
    "            \"id\": org_sent,\n",
    "            \"frames\": target_lemma_frames,  # list\n",
    "        })\n",
    "\n",
    "    output_df = pd.DataFrame(records, columns=[\"subfolder\", \"id\", \"frames\"])\n",
    "\n",
    "    # Keep only rows where frames is a non-empty list\n",
    "    output_df = output_df[\n",
    "        output_df[\"frames\"].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # Add target column as a 1-element list\n",
    "    output_df[\"target\"] = output_df.index.map(lambda _: [target_lemma])\n",
    "\n",
    "    # Explode frames: ['a','b'] -> two rows; then wrap each into ['a'], ['b']\n",
    "    output_df = output_df.explode(\"frames\", ignore_index=True)\n",
    "    output_df[\"frames\"] = output_df[\"frames\"].apply(\n",
    "        lambda x: [x] if pd.notna(x) else []\n",
    "    )\n",
    "\n",
    "    # (safety) drop any empty frames rows\n",
    "    output_df = output_df[output_df[\"frames\"].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76702ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def jsd_from_lists(list_p, list_q, log_base=2, min_freq=1):\n",
    "    \"\"\"\n",
    "    Jensen–Shannon divergence between two empirical distributions (lists of labels).\n",
    "\n",
    "    min_freq: keep items whose *combined* count across both periods is >= min_freq.\n",
    "              (i.e., count_p[item] + count_q[item] >= min_freq)\n",
    "\n",
    "    Returns: jsd, support, p, q\n",
    "    \"\"\"\n",
    "    c_p, c_q = Counter(list_p), Counter(list_q)\n",
    "\n",
    "    # joint support (before filtering)\n",
    "    support_all = set(c_p) | set(c_q)\n",
    "\n",
    "    # filter by combined frequency across both periods\n",
    "    if min_freq is None:\n",
    "        min_freq = 1\n",
    "    if min_freq < 1:\n",
    "        raise ValueError(\"min_freq must be >= 1 (or None).\")\n",
    "\n",
    "    support = sorted(\n",
    "        k for k in support_all\n",
    "        if (c_p.get(k, 0) + c_q.get(k, 0)) >= min_freq\n",
    "    )\n",
    "\n",
    "    # if nothing survives filtering, return NaN (or change to 0.0 if you prefer)\n",
    "    if not support:\n",
    "        return float(\"nan\"), [], np.array([]), np.array([])\n",
    "\n",
    "    p = np.array([c_p.get(k, 0) for k in support], dtype=float)\n",
    "    q = np.array([c_q.get(k, 0) for k in support], dtype=float)\n",
    "\n",
    "    sp, sq = p.sum(), q.sum()\n",
    "    \n",
    "    # if one side becomes empty after filtering, JSD is not well-defined as written\n",
    "    if sp == 0 or sq == 0:\n",
    "        return float(\"nan\"), support, np.array([]), np.array([])\n",
    "\n",
    "    p = p / sp\n",
    "    q = q / sq\n",
    "    m = 0.5 * (p + q)\n",
    "\n",
    "    def kl(a, b):\n",
    "        mask = a > 0\n",
    "        return np.sum(a[mask] * np.log(a[mask] / b[mask]))\n",
    "\n",
    "    jsd = 0.5 * kl(p, m) + 0.5 * kl(q, m)\n",
    "\n",
    "    if log_base == 2:\n",
    "        jsd = jsd / np.log(2)\n",
    "\n",
    "    return jsd, support, p, q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd75d45",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_lemmas = [\n",
    "        \"attack_nn\",\"bag_nn\",\"ball_nn\",\"bit_nn\",\"chairman_nn\",\"circle_vb\",\"contemplation_nn\",\"donkey_nn\",\n",
    "        \"edge_nn\",\"face_nn\",\"fiction_nn\",\"gas_nn\",\"graft_nn\",\"head_nn\",\"land_nn\",\"lane_nn\",\"lass_nn\",\n",
    "        \"multitude_nn\",\"ounce_nn\",\"part_nn\",\"pin_vb\",\"plane_nn\",\"player_nn\",\"prop_nn\",\"quilt_nn\",\"rag_nn\",\n",
    "        \"record_nn\",\"relationship_nn\",\"risk_nn\",\"savage_nn\",\"stab_nn\",\"stroke_vb\",\"thump_nn\",\"tip_vb\",\n",
    "        \"tree_nn\",\"twist_nn\",\"word_nn\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7472e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_results = {}\n",
    "\n",
    "for lemma in selected_lemmas:\n",
    "    # Load total results\n",
    "    total_results_path = f'... /{lemma}_lemma_FrameNet_parsed.jsonl' # <-- Replace with the actual path\n",
    "    total_results_df = pd.read_json(total_results_path, lines=True)\n",
    "    \n",
    "    # Get the distribution of frames\n",
    "    frames_distribution = get_frame_distribution(total_results_df, lemma)\n",
    "    get_frames_target(total_results_df, lemma, output_csv=f'... /{lemma}_lemma_frames.csv') # <-- Replace with the actual path\n",
    "    \n",
    "    # Calculate JSD for each each lemma\n",
    "    jsd, support, p, q = jsd_from_lists(\n",
    "        frames_distribution[\"corpus_1\"],\n",
    "        frames_distribution[\"corpus_2\"],\n",
    "        log_base=2,\n",
    "        )\n",
    "    jsd_results[lemma] = jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# --- inputs ---\n",
    "gold_path_task2 = Path(\"... /semeval2020_ulscd_eng/truth/graded.txt\") # <-- Replace with the actual path\n",
    "gold_path_task1 = Path(\"... /semeval2020_ulscd_eng/truth/binary.txt\") # <-- Replace with the actual path\n",
    "\n",
    "\n",
    "# Parse gold files\n",
    "gold_task2 = {}\n",
    "with gold_path_task2.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        k, v = line.split()[:2]\n",
    "        gold_task2[k] = float(v)\n",
    "\n",
    "gold_task1 = {}\n",
    "with gold_path_task1.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        k, v = line.split()[:2]\n",
    "        gold_task1[k] = int(float(v))  # robust if 0.0/1.0\n",
    "\n",
    "\n",
    "# Task2 Spearman (graded)\n",
    "keys2 = sorted(set(gold_task2) & set(jsd_results))\n",
    "x = [gold_task2[k] for k in keys2]\n",
    "y = [jsd_results[k] for k in keys2]\n",
    "rho, p = spearmanr(x, y)\n",
    "print(f\"[Task2] n={len(keys2)}  spearman_rho={rho:.6f}\")\n",
    "\n",
    "# Task1 Accuracy with fixed threshold: JSD >= 0.5 => 1 else 0\n",
    "thr = 0.5\n",
    "keys1 = sorted(gold_task1.keys())\n",
    "\n",
    "gold_bin = np.array([gold_task1[k] for k in keys1], dtype=int)\n",
    "preds = np.array([1 if jsd_results.get(k, float(\"-inf\")) >= thr else 0 for k in keys1], dtype=int)\n",
    "\n",
    "acc = (preds == gold_bin).mean()\n",
    "coverage = sum(k in jsd_results for k in keys1)\n",
    "print(f\"[Task1 JSD>=0.5] n={len(keys1)}  coverage={coverage}/{len(keys1)}  acc={acc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9717aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from math import ceil\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_scores_txt(path: str) -> dict[str, float]:\n",
    "    scores = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            line = line.strip(\"{} ,\")\n",
    "\n",
    "            parts = re.split(r\"\\s+\", line)\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            key = parts[0].strip().strip(\"',\\\"\")\n",
    "            val_str = parts[1].strip().strip(\"',\\\"\")\n",
    "\n",
    "            try:\n",
    "                scores[key] = float(val_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scores\n",
    "\n",
    "\n",
    "def rank_quadrants_from_two_scores(\n",
    "    pred_scores: Dict[str, float],     # e.g., jsd_scores\n",
    "    gold_scores: Dict[str, float],     # e.g., gold\n",
    "    top_frac: float = 0.30,\n",
    "    bottom_frac: float = 0.30,\n",
    "    rank_method: str = \"min\",\n",
    "    top_k_each: int = 10,\n",
    "    join: str = \"inner\",               # \"inner\" (default) or \"outer\"\n",
    ") -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Build TP/FP/FN/TN based on top/bottom regions of ranks in pred vs gold lists.\n",
    "\n",
    "    TP: pred TOP    & gold TOP\n",
    "    FP: pred TOP    & gold BOTTOM\n",
    "    FN: pred BOTTOM & gold TOP\n",
    "    TN: pred BOTTOM & gold BOTTOM\n",
    "    Others: NA (MID)\n",
    "    \"\"\"\n",
    "\n",
    "    if not (0 < top_frac <= 1) or not (0 < bottom_frac <= 1):\n",
    "        raise ValueError(\"top_frac and bottom_frac must be in (0, 1].\")\n",
    "    if top_frac + bottom_frac > 1:\n",
    "        raise ValueError(\"top_frac + bottom_frac must be <= 1 to keep non-overlapping regions.\")\n",
    "    if join not in {\"inner\", \"outer\"}:\n",
    "        raise ValueError(\"join must be 'inner' or 'outer'.\")\n",
    "\n",
    "    pred_s = pd.Series(pred_scores, dtype=\"float64\").rename(\"pred\")\n",
    "    gold_s = pd.Series(gold_scores, dtype=\"float64\").rename(\"gold\")\n",
    "\n",
    "    df = pd.concat([pred_s, gold_s], axis=1, join=join)\n",
    "\n",
    "    # If outer join, drop missing pairs (cannot rank compare)\n",
    "    df = df.dropna(subset=[\"pred\", \"gold\"]).copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), {\"TP\": [], \"FP\": [], \"FN\": [], \"TN\": []}\n",
    "\n",
    "    # Ranking (higher score => higher rank)\n",
    "    df[\"pred_rank\"] = df[\"pred\"].rank(ascending=False, method=rank_method)\n",
    "    df[\"gold_rank\"] = df[\"gold\"].rank(ascending=False, method=rank_method)\n",
    "\n",
    "    n = len(df)\n",
    "    top_n = max(1, ceil(top_frac * n))\n",
    "    bot_n = max(1, ceil(bottom_frac * n))\n",
    "    bot_start_rank = n - bot_n + 1\n",
    "\n",
    "    df[\"pred_top\"] = df[\"pred_rank\"] <= top_n\n",
    "    df[\"pred_bottom\"] = df[\"pred_rank\"] >= bot_start_rank\n",
    "    df[\"gold_top\"] = df[\"gold_rank\"] <= top_n\n",
    "    df[\"gold_bottom\"] = df[\"gold_rank\"] >= bot_start_rank\n",
    "\n",
    "    def _region(is_top: bool, is_bottom: bool) -> str:\n",
    "        if is_top:\n",
    "            return \"TOP\"\n",
    "        if is_bottom:\n",
    "            return \"BOTTOM\"\n",
    "        return \"MID\"\n",
    "\n",
    "    df[\"pred_region\"] = [_region(t, b) for t, b in zip(df[\"pred_top\"], df[\"pred_bottom\"])]\n",
    "    df[\"gold_region\"] = [_region(t, b) for t, b in zip(df[\"gold_top\"], df[\"gold_bottom\"])]\n",
    "\n",
    "    df[\"quadrant\"] = \"NA\"\n",
    "    df.loc[df[\"pred_top\"] & df[\"gold_top\"], \"quadrant\"] = \"TP\"\n",
    "    df.loc[df[\"pred_top\"] & df[\"gold_bottom\"], \"quadrant\"] = \"FP\"\n",
    "    df.loc[df[\"pred_bottom\"] & df[\"gold_top\"], \"quadrant\"] = \"FN\"\n",
    "    df.loc[df[\"pred_bottom\"] & df[\"gold_bottom\"], \"quadrant\"] = \"TN\"\n",
    "\n",
    "    # Diagnostics\n",
    "    df[\"rank_gap_signed\"] = df[\"pred_rank\"] - df[\"gold_rank\"]\n",
    "    df[\"rank_gap_abs\"] = df[\"rank_gap_signed\"].abs()\n",
    "    df[\"rank_sum\"] = df[\"pred_rank\"] + df[\"gold_rank\"]\n",
    "\n",
    "    # Build example lists\n",
    "    tp = df[df[\"quadrant\"] == \"TP\"].sort_values([\"rank_sum\", \"pred_rank\", \"gold_rank\"]).index.tolist()\n",
    "    fp = df[df[\"quadrant\"] == \"FP\"].sort_values([\"pred_rank\", \"gold_rank\"], ascending=[True, False]).index.tolist()\n",
    "    fn = df[df[\"quadrant\"] == \"FN\"].sort_values([\"gold_rank\", \"pred_rank\"], ascending=[True, False]).index.tolist()\n",
    "    tn = df[df[\"quadrant\"] == \"TN\"].sort_values([\"rank_sum\", \"pred_rank\", \"gold_rank\"], ascending=[False, False, False]).index.tolist()\n",
    "\n",
    "    groups = {\n",
    "        \"TP\": tp[:top_k_each],\n",
    "        \"FP\": fp[:top_k_each],\n",
    "        \"FN\": fn[:top_k_each],\n",
    "        \"TN\": tn[:top_k_each],\n",
    "    }\n",
    "\n",
    "    # Optional: sort for readability\n",
    "    df = df.sort_values([\"pred_rank\", \"gold_rank\"])\n",
    "\n",
    "    return df, groups\n",
    "\n",
    "gold_scores = load_scores_txt(gold_path_task2)  \n",
    "df_q, groups = rank_quadrants_from_two_scores(jsd_results, gold_scores, top_frac=0.3, bottom_frac=0.3)\n",
    "print(df_q[[\"pred\",\"gold\",\"pred_rank\",\"gold_rank\",\"pred_region\",\"gold_region\",\"quadrant\"]])\n",
    "print(groups)\n",
    "print(df_q[\"quadrant\"].value_counts())\n",
    "missing_in_gold = set(jsd_results) - set(gold_scores)\n",
    "missing_in_pred = set(gold_scores) - set(jsd_results)\n",
    "print(\"missing_in_gold:\", sorted(missing_in_gold))\n",
    "print(\"missing_in_pred:\", sorted(missing_in_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bython311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
